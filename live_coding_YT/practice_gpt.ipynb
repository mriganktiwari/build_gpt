{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5feb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../gpt/input.txt'\n",
    "text = open(data_file, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e906410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4a0dd",
   "metadata": {},
   "source": [
    "## testing tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56a45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tok = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7b812f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6151, 1070, 758]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.encode('hi there !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55211b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hi'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode_single_token_bytes(6151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b01e3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' there'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode_single_token_bytes(1070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe12cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' !'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode_single_token_bytes(758)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926537dc",
   "metadata": {},
   "source": [
    "## `char` level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3275b0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0f35d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(vocab)}\n",
    "itos = {i:ch for i,ch in enumerate(vocab)}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d57674a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda text: [stoi[ch] for ch in text]\n",
    "decode = lambda tokens: ''.join([itos[ix] for ix in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "709992e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text))\n",
    "print(data[:10])\n",
    "print(data.dtype)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd50ac",
   "metadata": {},
   "source": [
    "## visualizing a chunk of text in encoded format with targets for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1fcf6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = data[:block_size]\n",
    "y = data[1:block_size+1] # shifted to right by 1, from x\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b20c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([18]) --> target 47\n",
      "input tensor([18, 47]) --> target 56\n",
      "input tensor([18, 47, 56]) --> target 57\n",
      "input tensor([18, 47, 56, 57]) --> target 58\n",
      "input tensor([18, 47, 56, 57, 58]) --> target 1\n",
      "input tensor([18, 47, 56, 57, 58,  1]) --> target 15\n",
      "input tensor([18, 47, 56, 57, 58,  1, 15]) --> target 47\n",
      "input tensor([18, 47, 56, 57, 58,  1, 15, 47]) --> target 58\n"
     ]
    }
   ],
   "source": [
    "for i in range(block_size):\n",
    "    print(f'input {x[:i+1]} --> target {y[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadd509",
   "metadata": {},
   "source": [
    "### introduce `batch` dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1fa837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split, block_size=8, batch_size=8, device='cpu'):\n",
    "    data = train if split == 'train' else test\n",
    "    ix = torch.randint(high=len(data)-block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i   : i   + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i+1 + block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "x,y = get_batch('train', block_size=8, batch_size=4)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11719551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([24]) : target is 43\n",
      "when input is tensor([24, 43]) : target is 58\n",
      "when input is tensor([24, 43, 58]) : target is 5\n",
      "when input is tensor([24, 43, 58,  5]) : target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]) : target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) : target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) : target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) : target is 39\n",
      "when input is tensor([44]) : target is 53\n",
      "when input is tensor([44, 53]) : target is 56\n",
      "when input is tensor([44, 53, 56]) : target is 1\n",
      "when input is tensor([44, 53, 56,  1]) : target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]) : target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) : target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) : target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) : target is 1\n",
      "when input is tensor([52]) : target is 58\n",
      "when input is tensor([52, 58]) : target is 1\n",
      "when input is tensor([52, 58,  1]) : target is 58\n",
      "when input is tensor([52, 58,  1, 58]) : target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]) : target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) : target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) : target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) : target is 46\n",
      "when input is tensor([25]) : target is 17\n",
      "when input is tensor([25, 17]) : target is 27\n",
      "when input is tensor([25, 17, 27]) : target is 10\n",
      "when input is tensor([25, 17, 27, 10]) : target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]) : target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) : target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) : target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) : target is 39\n"
     ]
    }
   ],
   "source": [
    "# this is how inputs to a GPT model would look like\n",
    "# context-length ranging from 1-block_size\n",
    "\n",
    "B,T = x.shape\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = x[b, :t+1]\n",
    "        target =  y[b, t  ]\n",
    "        print(f'when input is {context} : target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb1a7d",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "## Bigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7defc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.tok_embedding(idx) # (B,T,vocab_size)\n",
    "\n",
    "        B,T,C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T,vocab_size), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # ids: B,T\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,_ = self(idx) # (B,T,C)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, new_idx), dim=1) # (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4675ead1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n",
      "tensor([[ 0, 50, 44, 22, 43, 59, 49, 30, 59, 39, 30, 22, 23, 36, 13, 37, 58, 36,\n",
      "         64, 44, 22, 10, 20, 17, 28, 47, 59,  7,  7, 57, 16, 47, 53, 47, 11, 21,\n",
      "         24, 15, 53,  9, 54, 20, 26, 32, 51, 16, 61, 22, 57, 44, 46, 43, 23, 30,\n",
      "         62, 38, 15, 18, 57,  0, 50, 38, 22,  1, 36, 29, 41, 12, 10, 57, 10, 20,\n",
      "         17, 64, 17, 52, 36, 39, 50, 17, 28, 49, 50, 41, 28, 33,  1, 41, 24,  5,\n",
      "         16, 54, 42, 24, 15, 39, 44, 14, 46, 43, 20]])\n",
      "\n",
      "lfJeukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
      "lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n"
     ]
    }
   ],
   "source": [
    "model = BigramLM()\n",
    "logits, loss = model(x,y)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(idx)\n",
    "idx = model.generate(idx, max_new_tokens=100)\n",
    "print(idx)\n",
    "\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cffad8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8ae4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(4.8740, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1951, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3270, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0485, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0234, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6585, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5237, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6933, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5405, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.2774, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "block_size=8\n",
    "batch_size=4\n",
    "for iter in range(1000):\n",
    "    xb,yb = get_batch('train', block_size=block_size, batch_size=batch_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 100 == 0:\n",
    "        print(f'{loss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c80bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NqBfe o m!d omry-mo y an?k't d looSvar thavend tthanI pt BYQkir\n",
      "N\n",
      "kink llll heescerge.\n",
      "As\n",
      "Wim heANve\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "idx = model.generate(idx, max_new_tokens=100)\n",
    "\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc38561",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "## let's build some diversion and add positional embeddings, for better modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "507adba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram LM\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        tok_emb = self.tok_embedding(idx) # (B,T,vocab_size)\n",
    "        logits = self.lm_head(tok_emb)\n",
    "\n",
    "        B,T,C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T,vocab_size), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # ids: B,T\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,_ = self(idx) # (B,T,C)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, new_idx), dim=1) # (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5dd5312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(2.5480, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7870, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6660, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.4883, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7111, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.4736, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.3161, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.4362, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.3136, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5640, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "block_size=8\n",
    "batch_size=4\n",
    "for iter in range(1000):\n",
    "    xb,yb = get_batch('train', block_size=block_size, batch_size=batch_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 100 == 0:\n",
    "        print(f'{loss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37e67e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bon ntrete .\n",
      "th havef i$MWingithacu ind't ho I hamyon!'s\n",
      "\n",
      "\n",
      "ANESTHite:\n",
      "MMICLIfiror tlld chaitarslsen'\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "idx = model.generate(idx, max_new_tokens=100)\n",
    "\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e8df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "device = \"mps\"# if torch.backends.mps.is_available() else \"cpu\"\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x,y = get_batch(split, device=device)\n",
    "            _,loss = model(x,y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7866fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BigramLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m batch_size=\u001b[32m4\u001b[39m\n\u001b[32m      3\u001b[39m n_embd = \u001b[32m16\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mBigramLM\u001b[49m(n_embd)\n\u001b[32m      6\u001b[39m model = model.to(device)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m([p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m params\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'BigramLM' is not defined"
     ]
    }
   ],
   "source": [
    "block_size=8\n",
    "batch_size=4\n",
    "n_embd = 16\n",
    "\n",
    "model = BigramLM(n_embd)\n",
    "model = model.to(device)\n",
    "print(f'{sum([p.numel() for p in model.parameters()])} params')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3)\n",
    "\n",
    "for iter in range(1000):\n",
    "    xb,yb = get_batch('train', block_size=block_size, batch_size=batch_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 100 == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f'iter: {iter} | train loss = {out['train']:.4f} | test loss = {out['test']:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d245a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uoasthapdse tizenderst els yu frnie hy:\n",
      "\n",
      "\n",
      "Hak, COI teg aglellthorr gtecowor hend ge?\n",
      "Ten, reothakech\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "idx = model.generate(idx, max_new_tokens=100)\n",
    "\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190fd1d6",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b77ee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0d84889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2273 params\n",
      "iter: 0 | train loss = 4.6241 | test loss = 4.6222\n",
      "iter: 100 | train loss = 3.2371 | test loss = 3.2595\n",
      "iter: 200 | train loss = 2.9886 | test loss = 2.9884\n",
      "iter: 300 | train loss = 2.8827 | test loss = 2.8730\n",
      "iter: 400 | train loss = 2.7515 | test loss = 2.7698\n",
      "iter: 500 | train loss = 2.7015 | test loss = 2.7233\n",
      "iter: 600 | train loss = 2.6965 | test loss = 2.6799\n",
      "iter: 700 | train loss = 2.6509 | test loss = 2.6503\n",
      "iter: 800 | train loss = 2.6213 | test loss = 2.6359\n",
      "iter: 900 | train loss = 2.6014 | test loss = 2.6268\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.tok_embedding(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device=device)) # (T,n_embd)\n",
    "        logits = self.lm_head(tok_emb + pos_emb)\n",
    "\n",
    "        B,T,C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T,vocab_size), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B,T\n",
    "        B,T = idx.shape\n",
    "        idx_chopped = idx[:, -block_size:]\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,_ = self(idx_chopped) # (B,T,C)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, new_idx), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "block_size=8\n",
    "batch_size=4\n",
    "n_embd = 16\n",
    "\n",
    "model = GPT(n_embd)\n",
    "print(f'{sum([p.numel() for p in model.parameters()])} params')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3)\n",
    "\n",
    "for iter in range(1000):\n",
    "    xb,yb = get_batch('train', block_size=block_size, batch_size=batch_size)\n",
    "    xb,yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 100 == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f'iter: {iter} | train loss = {out['train']:.4f} | test loss = {out['test']:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82d1ab4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HPWLAHPYD\n",
      "p\n",
      "IMIAPDGAA IF\n",
      "S\n",
      "bTGDTDTAMDYTI\n",
      "TAkFT\n",
      "NAI-hDN\n",
      "L:\n",
      "\n",
      "OTAAGSAIASO\n",
      " N\n",
      "FWBAAIFADAIFAIBTT\n",
      "!'AcC\n",
      "AA\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "idx = model.generate(idx, max_new_tokens=100)\n",
    "\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a49bbc",
   "metadata": {},
   "source": [
    "## Self-attention tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25643b6a",
   "metadata": {},
   "source": [
    "### 1. Basic idea\n",
    "- an element in a batch has to become `block_size` distinct examples\n",
    "- incrementaly the examples will **aggregate** information from all previously occuring tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93e67c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 2,4,2\n",
    "\n",
    "x = torch.randn((B,T,C))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98cd6597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5296, -0.0165],\n",
       "         [-1.1309, -0.1778],\n",
       "         [-0.5091, -2.4500],\n",
       "         [-1.2056,  0.2115]],\n",
       "\n",
       "        [[ 1.7162, -0.5178],\n",
       "         [-1.6298, -1.5357],\n",
       "         [ 0.1144,  1.0883],\n",
       "         [ 0.5618, -0.3086]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c05f163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5296, -0.0165],\n",
       "         [-0.8303, -0.0972],\n",
       "         [-0.7232, -0.8814],\n",
       "         [-0.8438, -0.6082]],\n",
       "\n",
       "        [[ 1.7162, -0.5178],\n",
       "         [ 0.0432, -1.0267],\n",
       "         [ 0.0669, -0.3217],\n",
       "         [ 0.1906, -0.3184]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = torch.zeros_like(x)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        prev = x[b, :t+1]\n",
    "        xbow[b, t] = prev.mean(dim=0)\n",
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb88cb",
   "metadata": {},
   "source": [
    "### 2. Matrix multiply using a triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc03bf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tril(torch.ones((T,T)))\n",
    "a = a / a.sum(dim=1, keepdim=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28e12317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5296, -0.0165],\n",
       "         [-0.8303, -0.0972],\n",
       "         [-0.7232, -0.8814],\n",
       "         [-0.8438, -0.6082]],\n",
       "\n",
       "        [[ 1.7162, -0.5178],\n",
       "         [ 0.0432, -1.0267],\n",
       "         [ 0.0669, -0.3217],\n",
       "         [ 0.1906, -0.3184]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = a @ x # (t,t) @ (b,t,c) -> (_,t,t) @ (b,t,c) -> (b,t,c)\n",
    "xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdf1dee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5f7ff",
   "metadata": {},
   "source": [
    "### 3. Using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdd7d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones((T,T)))\n",
    "\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "xbow3 = wei @ x # (t,t) @ (b,t,c) -> (b,t,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a264eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb251c8",
   "metadata": {},
   "source": [
    "### 4. self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "195c4063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_size = 16\n",
    "\n",
    "query = nn.Linear(C, head_size)\n",
    "key = nn.Linear(C, head_size)\n",
    "value = nn.Linear(C, head_size)\n",
    "\n",
    "q = query(x) # (b,t,head_size)\n",
    "k = key(x) # (b,t,head_size)\n",
    "v = value(x) # (b,t,head_size)\n",
    "\n",
    "wei = (q @ k.transpose(-1,-2)) / head_size ** 0.5 # (b,t,head_size) @ (b,head_size,t) -> (b,t,t)\n",
    "\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ v # (b,t,t) @ (b,t,head_size) -> (b,t,head_size)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e27be28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4843, 0.5157, 0.0000, 0.0000],\n",
       "         [0.3075, 0.5056, 0.1868, 0.0000],\n",
       "         [0.2823, 0.2793, 0.1133, 0.3251]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1683, 0.8317, 0.0000, 0.0000],\n",
       "         [0.4849, 0.2289, 0.2862, 0.0000],\n",
       "         [0.1907, 0.4075, 0.1803, 0.2215]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c853d6",
   "metadata": {},
   "source": [
    "## Plugging \"self-attention\" in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "968f6950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7553 params\n",
      "iter: 0 | train loss = 4.1184 | test loss = 4.1204\n",
      "iter: 1000 | train loss = 2.4777 | test loss = 2.4819\n",
      "iter: 2000 | train loss = 2.4394 | test loss = 2.4265\n",
      "iter: 3000 | train loss = 2.3896 | test loss = 2.4276\n",
      "iter: 4000 | train loss = 2.4021 | test loss = 2.4174\n",
      "\n",
      "NAAFT\n",
      "TSKTIONTRFSIDTTIEUWLHAWLSAQAnTTICCLFAA'AILBTITgWALHREQGTASTPLNLMnFCWSBIIRBLTBGBCADBlB\n",
      "TQHuCAiS\n"
     ]
    }
   ],
   "source": [
    "block_size=8\n",
    "batch_size=32\n",
    "n_embd = 32\n",
    "# head_size = 16\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (b,t,head_size)\n",
    "        q = self.query(x) # (b,t,head_size)\n",
    "        v = self.value(x) # (b,t,head_size)\n",
    "\n",
    "        wei = (q @ k.transpose(-1,-2)) / head_size ** 0.5 # (b,t,head_size) @ (b,head_size,t) -> (b,t,t)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        out = wei @ v # (b,t,t) @ (b,t,head_size) -> (b,t,head_size)\n",
    "        return out\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.tok_embedding(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device=device)) # (T,n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        B,T,C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T,vocab_size), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B,T\n",
    "        B,T = idx.shape\n",
    "        idx_chopped = idx[:, -block_size:]\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,_ = self(idx_chopped) # (B,T,C)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, new_idx), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPT(n_embd)\n",
    "print(f'{sum([p.numel() for p in model.parameters()])} params')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "max_iters = 5000\n",
    "for iter in range(max_iters):\n",
    "    xb,yb = get_batch('train', block_size=block_size, batch_size=batch_size)\n",
    "    xb,yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 1000 == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f'iter: {iter} | train loss = {out['train']:.4f} | test loss = {out['test']:.4f}')\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "idx = model.generate(idx, max_new_tokens=100)\n",
    "\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5951fc",
   "metadata": {},
   "source": [
    "## Plug \"multi-headed attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44329e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7553 params\n",
      "iter: 0 | train loss = 3.9767 | test loss = 3.9892\n",
      "iter: 1000 | train loss = 2.3291 | test loss = 2.3421\n",
      "iter: 2000 | train loss = 2.2475 | test loss = 2.2983\n",
      "iter: 3000 | train loss = 2.2000 | test loss = 2.2958\n",
      "iter: 4000 | train loss = 2.1946 | test loss = 2.2668\n",
      "\n",
      "IWMWSrSMUO\n",
      "\n",
      "ICCTmc\n",
      "\n",
      "JETEBWTIPIBS\n",
      "IE\n",
      "mTbOTEBYLBPGAmOHoVH\n",
      "ABVBfTTtI\n",
      "\n",
      "MNWIMBHBfFPBM\n",
      "ABP\n",
      "GHLIDAATRAHMmTH\n"
     ]
    }
   ],
   "source": [
    "block_size=8\n",
    "batch_size=32\n",
    "n_embd = 32\n",
    "# head_size = 16\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.heads(x)\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (b,t,head_size)\n",
    "        q = self.query(x) # (b,t,head_size)\n",
    "        v = self.value(x) # (b,t,head_size)\n",
    "\n",
    "        wei = (q @ k.transpose(-1,-2)) / head_size ** 0.5 # (b,t,head_size) @ (b,head_size,t) -> (b,t,t)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        out = wei @ v # (b,t,t) @ (b,t,head_size) -> (b,t,head_size)\n",
    "        return out\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadedAttention(4, n_embd//4)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.tok_embedding(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device=device)) # (T,n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        B,T,C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T,vocab_size), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B,T\n",
    "        B,T = idx.shape\n",
    "        idx_chopped = idx[:, -block_size:]\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,_ = self(idx_chopped) # (B,T,C)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, new_idx), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPT(n_embd)\n",
    "print(f'{sum([p.numel() for p in model.parameters()])} params')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "max_iters = 5000\n",
    "for iter in range(max_iters):\n",
    "    xb,yb = get_batch('train', block_size=block_size, batch_size=batch_size)\n",
    "    xb,yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 1000 == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f'iter: {iter} | train loss = {out['train']:.4f} | test loss = {out['test']:.4f}')\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "idx = model.generate(idx, max_new_tokens=100)\n",
    "\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c33039",
   "metadata": {},
   "source": [
    "## Plug in \"feed forward network\" to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b92ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15905 params\n",
      "iter: 0 | train loss = 4.1050 | test loss = 4.1043\n",
      "iter: 1000 | train loss = 2.2440 | test loss = 2.2881\n",
      "iter: 2000 | train loss = 2.1746 | test loss = 2.2144\n",
      "iter: 3000 | train loss = 2.1351 | test loss = 2.2291\n",
      "iter: 4000 | train loss = 2.1139 | test loss = 2.2423\n",
      "\n",
      "FEYTSpTTECrWHIADJYNDDWMDTAIESYTWSSTTSG\n",
      "-UAAQs NwNNFTOKQyFABeTTMeNQEEITAUyBGMj,HKVFhiOWrHFe.F IJASWyY\n"
     ]
    }
   ],
   "source": [
    "block_size=8\n",
    "batch_size=32\n",
    "n_embd = 32\n",
    "# head_size = 16\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.heads(x)\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (b,t,head_size)\n",
    "        q = self.query(x) # (b,t,head_size)\n",
    "        v = self.value(x) # (b,t,head_size)\n",
    "\n",
    "        wei = (q @ k.transpose(-1,-2)) / head_size ** 0.5 # (b,t,head_size) @ (b,head_size,t) -> (b,t,t)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        out = wei @ v # (b,t,t) @ (b,t,head_size) -> (b,t,head_size)\n",
    "        return out\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadedAttention(4, n_embd//4)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.tok_embedding(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device=device)) # (T,n_embd)\n",
    "        x = tok_emb + pos_emb # (b,t,n_embd)\n",
    "        x = self.sa_heads(x) # (b,t,n_embd)\n",
    "        x = self.ffwd(x) # (b,t,n_embd)\n",
    "        logits = self.lm_head(x) # (b,t,vocab_size)\n",
    "\n",
    "        B,T,C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T,vocab_size), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B,T\n",
    "        B,T = idx.shape\n",
    "        idx_chopped = idx[:, -block_size:]\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,_ = self(idx_chopped) # (B,T,C)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, new_idx), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPT(n_embd)\n",
    "print(f'{sum([p.numel() for p in model.parameters()])} params')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "max_iters = 5000\n",
    "for iter in range(max_iters):\n",
    "    xb,yb = get_batch('train', block_size=block_size, batch_size=batch_size)\n",
    "    xb,yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 1000 == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f'iter: {iter} | train loss = {out['train']:.4f} | test loss = {out['test']:.4f}')\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "idx = model.generate(idx, max_new_tokens=100)\n",
    "\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60484add",
   "metadata": {},
   "source": [
    "## Add MHA & FFWD to `Block`; and  plug in \"Residual connections\" within blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012dd298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80128 params\n",
      "iter: 0 | train loss = 4.2674 | test loss = 4.2777\n",
      "iter: 500 | train loss = 2.6528 | test loss = 2.6731\n",
      "iter: 1000 | train loss = 2.4729 | test loss = 2.4817\n",
      "iter: 1500 | train loss = 2.3998 | test loss = 2.4054\n",
      "iter: 2000 | train loss = 2.3446 | test loss = 2.3535\n",
      "iter: 2500 | train loss = 2.2893 | test loss = 2.3150\n",
      "iter: 3000 | train loss = 2.2585 | test loss = 2.2842\n",
      "iter: 3500 | train loss = 2.2332 | test loss = 2.2431\n",
      "iter: 4000 | train loss = 2.2107 | test loss = 2.2248\n",
      "iter: 4500 | train loss = 2.1992 | test loss = 2.2027\n",
      "tensor([ 0,  1, 47, 52, 44, 59, 53, 56,  1, 47, 57,  1, 57, 43, 39, 50, 50,  1,\n",
      "        51, 43,  8,  1, 35, 53, 56, 42,  1, 51, 39, 41, 43,  1, 41, 46, 39, 58,\n",
      "         8,  0,  0, 16, 33, 23, 21, 38, 13, 16, 53, 50, 50,  1, 47, 52,  1, 53,\n",
      "        44,  1, 58, 46, 39, 57,  1, 58, 53,  1, 39, 56, 42,  1, 63, 53, 59, 56,\n",
      "         1, 19, 53, 44,  1, 39, 57, 61, 43, 52,  1, 45, 53,  1, 46, 43,  1, 56,\n",
      "        43,  5, 42,  1, 56, 43, 44, 53, 52,  1, 58], device='mps:0')\n",
      "\n",
      " infuor is seall me. Word mace chat.\n",
      "\n",
      "DUKIZADoll in of thas to ard your Gof aswen go he re'd refon t\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "block_size=256\n",
    "batch_size=64\n",
    "n_embd = 384\n",
    "n_heads = 6\n",
    "n_layer = 12\n",
    "dropout = 0.2\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else test\n",
    "    ix = torch.randint(high=len(data)-block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i   : i   + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i+1 + block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters, device=device)\n",
    "        for k in range(eval_iters):\n",
    "            x,y = get_batch(split)\n",
    "            _,loss = model(x,y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, n_embd):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.heads = MultiHeadedAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.heads(self.ln1(x)) # (b,t,n_embd)\n",
    "        x = x + self.ffwd(self.ln2(x)) # (b,t,n_embd)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.heads(x)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (b,t,n_embd)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out  # (b,t,n_embd)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (b,t,head_size)\n",
    "        q = self.query(x) # (b,t,head_size)\n",
    "        v = self.value(x) # (b,t,head_size)\n",
    "\n",
    "        # wei = (q @ k.transpose(-2,-1)) * self.head_size ** -0.5 # (b,t,head_size) @ (b,head_size,t) -> (b,t,t)\n",
    "        wei = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.head_size) # (b,t,t)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v # (b,t,t) @ (b,t,head_size) -> (b,t,head_size)\n",
    "        return out\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "        blocks = [Block(n_heads, n_embd) for _ in range(n_layer)]\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.tok_embedding(idx) # (B,T,n_embd)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device=idx.device)) # (T,n_embd)\n",
    "        x = tok_emb + pos_emb # (b,t,n_embd)\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(self.ln_f(x)) # (b,t,vocab_size)x\n",
    "\n",
    "        B,T,C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T,vocab_size), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B,T\n",
    "        B,T = idx.shape\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_chopped = idx[:, -block_size:]\n",
    "            logits,_ = self(idx_chopped) # (B,T,C)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, new_idx), dim=1) # (B,T+1)\n",
    "        self.train()\n",
    "        return idx\n",
    "\n",
    "model = GPT()\n",
    "print(f'{sum([p.numel() for p in model.parameters()])} params')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "max_iters = 5000\n",
    "for iter in range(max_iters):\n",
    "    xb,yb = get_batch('train')\n",
    "    # xb,yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % eval_interval == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f'iter: {iter} | train loss = {out['train']:.4f} | test loss = {out['test']:.4f}')\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "idx = model.generate(idx, max_new_tokens=500)\n",
    "print(decode(idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76872c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

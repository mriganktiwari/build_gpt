{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53896b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8028a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = open('../gpt/input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c9c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c018e4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(''.join(shakespeare))))\n",
    "# print(''.join(vocab))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bfd0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(vocab)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "\n",
    "# encode list of characters to list of integers\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "# encode('Hello')\n",
    "\n",
    "# decode list of int to list of chars\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "# decode(encode('Hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7f309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(shakespeare))\n",
    "len(data) == len(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8224c572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ce7e22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]), 'First Citi')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10], shakespeare[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c1a8264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "--------------------\n",
      "tensor([18]) --> 47\n",
      "tensor([18, 47]) --> 56\n",
      "tensor([18, 47, 56]) --> 57\n",
      "tensor([18, 47, 56, 57]) --> 58\n",
      "tensor([18, 47, 56, 57, 58]) --> 1\n",
      "tensor([18, 47, 56, 57, 58,  1]) --> 15\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) --> 47\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) --> 58\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs n outputs\n",
    "\n",
    "block_size = 8 # time dimension\n",
    "# [18, 47, 56, 57]  --> [58]\n",
    "# [18]              --> [47]\n",
    "# [18, 47]          --> [56]\n",
    "# [18, 47, 56]      --> [57]\n",
    "\n",
    "x, y = data[:block_size], data[1:block_size+1]\n",
    "print(x, y)\n",
    "print('-'*20)\n",
    "for t in range(block_size):\n",
    "    inp = x[:t+1]\n",
    "    out = y[t]\n",
    "    print(f'{inp} --> {out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "216cba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[43, 12,  0,  0, 23, 21, 26, 19],\n",
      "        [43,  1, 39, 57,  1, 39,  1, 50],\n",
      "        [47, 52,  6,  1, 59, 54, 11,  1],\n",
      "        [ 6,  0, 13, 52, 42,  1, 40, 39]]) \n",
      " tensor([[12,  0,  0, 23, 21, 26, 19,  1],\n",
      "        [ 1, 39, 57,  1, 39,  1, 50, 39],\n",
      "        [52,  6,  1, 59, 54, 11,  1, 63],\n",
      "        [ 0, 13, 52, 42,  1, 40, 39, 49]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "ix = torch.randint(0, len(data), (batch_size,))\n",
    "x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
    "y = torch.stack([data[i+1 : i+1 + block_size] for i in ix], dim=0)\n",
    "print(x, '\\n' ,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f11621de",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data)*0.9)\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccdee74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[52,  1, 54, 56, 47, 57, 53, 52],\n",
      "        [43, 39, 42, 11,  1, 63, 53, 59],\n",
      "        [59, 50, 42,  1, 58, 46, 47, 57],\n",
      "        [12,  0,  0, 16, 33, 23, 17,  1]]) \n",
      " tensor([[ 1, 54, 56, 47, 57, 53, 52,  6],\n",
      "        [39, 42, 11,  1, 63, 53, 59, 56],\n",
      "        [50, 42,  1, 58, 46, 47, 57,  1],\n",
      "        [ 0,  0, 16, 33, 23, 17,  1, 27]])\n",
      "\n",
      "tensor([52]) --> 1\n",
      "tensor([52,  1]) --> 54\n",
      "tensor([52,  1, 54]) --> 56\n",
      "tensor([52,  1, 54, 56]) --> 47\n",
      "tensor([52,  1, 54, 56, 47]) --> 57\n",
      "tensor([52,  1, 54, 56, 47, 57]) --> 53\n",
      "tensor([52,  1, 54, 56, 47, 57, 53]) --> 52\n",
      "tensor([52,  1, 54, 56, 47, 57, 53, 52]) --> 6\n",
      "tensor([43]) --> 39\n",
      "tensor([43, 39]) --> 42\n",
      "tensor([43, 39, 42]) --> 11\n",
      "tensor([43, 39, 42, 11]) --> 1\n",
      "tensor([43, 39, 42, 11,  1]) --> 63\n",
      "tensor([43, 39, 42, 11,  1, 63]) --> 53\n",
      "tensor([43, 39, 42, 11,  1, 63, 53]) --> 59\n",
      "tensor([43, 39, 42, 11,  1, 63, 53, 59]) --> 56\n",
      "tensor([59]) --> 50\n",
      "tensor([59, 50]) --> 42\n",
      "tensor([59, 50, 42]) --> 1\n",
      "tensor([59, 50, 42,  1]) --> 58\n",
      "tensor([59, 50, 42,  1, 58]) --> 46\n",
      "tensor([59, 50, 42,  1, 58, 46]) --> 47\n",
      "tensor([59, 50, 42,  1, 58, 46, 47]) --> 57\n",
      "tensor([59, 50, 42,  1, 58, 46, 47, 57]) --> 1\n",
      "tensor([12]) --> 0\n",
      "tensor([12,  0]) --> 0\n",
      "tensor([12,  0,  0]) --> 16\n",
      "tensor([12,  0,  0, 16]) --> 33\n",
      "tensor([12,  0,  0, 16, 33]) --> 23\n",
      "tensor([12,  0,  0, 16, 33, 23]) --> 17\n",
      "tensor([12,  0,  0, 16, 33, 23, 17]) --> 1\n",
      "tensor([12,  0,  0, 16, 33, 23, 17,  1]) --> 27\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, bs):\n",
    "    d = train if split=='train' else val\n",
    "    ix = torch.randint(0, len(d) - block_size, (batch_size,))\n",
    "    xb = torch.stack([d[i   : i +   block_size] for i in ix], dim=0)\n",
    "    yb = torch.stack([d[i+1 : i+1 + block_size] for i in ix], dim=0)\n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch('train', bs=batch_size)\n",
    "print(xb, '\\n', yb)\n",
    "print()\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        print(f'{xb[b, :t+1]} --> {yb[b, t]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c95798f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fLHubkquuedbo'heYWmxWefLguwcIWHXHEM;Ej!xdmWpmRU 3'UDp'CGfSlKEvGUuPNuTtJKxlT3'dN;GR.OuVpEyTArOarqICq:\n"
     ]
    }
   ],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        logits = self.token_embedding_table(x) # (b, t, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # targets: (b, t)                                   --> (b*t)\n",
    "            # logits: (b, t, c) - c: channels --> (b, c, t)     --> (b*t, c)\n",
    "            # loss = F.cross_entropy(logits.transpose(-1,-2), targets)\n",
    "\n",
    "            B,T,C = logits.shape\n",
    "            logits_new = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits_new, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        # idx: (b, t)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx) # (b, t, vocab_size)\n",
    "            logits = logits[:, -1, :] # (b, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (b, vocab_size)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_new), dim=-1) # (b, t+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLM()\n",
    "logits, loss = model(xb, yb)\n",
    "# loss.item()\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(model.generate(idx)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b694a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac4e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item() = 2.4846417903900146\n",
      "\n",
      "TCFOMErey sthdishee so w d adoustrgameres my therat hy Le wad t ngfe hyowangoupo:\n",
      "BRCEYCxs.\n",
      "ANus; ur\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10000\n",
    "batch_size = 32\n",
    "\n",
    "for _ in range(max_iters):\n",
    "    # get a batch\n",
    "    xb, yb = get_batch('train', bs=batch_size)\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # set grad to None and backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f'{loss.item() = }')\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(model.generate(idx)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683282c",
   "metadata": {},
   "source": [
    "# Adding a diversion of `n_embd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "237db5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 250\n",
    "\n",
    "# evaluate loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split, bs=batch_size)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44a9251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        tok_emb = self.token_embedding_table(x) # (b, t, n_embd)\n",
    "        logits = self.lm_head(tok_emb) # (b, t, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # targets: (b, t)                                   --> (b*t)\n",
    "            # logits: (b, t, c) - c: channels --> (b, c, t)     --> (b*t, c)\n",
    "            # loss = F.cross_entropy(logits.transpose(-1,-2), targets)\n",
    "\n",
    "            B,T,C = logits.shape\n",
    "            logits_new = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits_new, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        # idx: (b, t)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx) # (b, vocab_size)\n",
    "            logits = logits[:, -1, :] # (b, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (b, vocab_size)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_new), dim=-1) # (b, t+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLM()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbaef305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 : Train Loss = 4.3357, Validation Loss = 4.3406\n",
      "Iteration 1001 : Train Loss = 2.5704, Validation Loss = 2.5716\n",
      "Iteration 2001 : Train Loss = 2.5110, Validation Loss = 2.5100\n",
      "Iteration 3001 : Train Loss = 2.4873, Validation Loss = 2.5049\n",
      "Iteration 4001 : Train Loss = 2.4798, Validation Loss = 2.4967\n",
      "Iteration 5001 : Train Loss = 2.4764, Validation Loss = 2.4949\n",
      "Iteration 6001 : Train Loss = 2.4661, Validation Loss = 2.4998\n",
      "Iteration 7001 : Train Loss = 2.4687, Validation Loss = 2.4924\n",
      "Iteration 8001 : Train Loss = 2.4629, Validation Loss = 2.4920\n",
      "Iteration 9001 : Train Loss = 2.4697, Validation Loss = 2.4860\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10000\n",
    "batch_size = 32\n",
    "eval_interval = 1000\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # get a batch\n",
    "    xb, yb = get_batch('train', bs=batch_size)\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f'Iteration {iter + 1} : Train Loss = {out['train']:.4f}, Validation Loss = {out['val']:.4f}')\n",
    "\n",
    "    # set grad to None and backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dcdceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "int wind.\n",
      "bu oure paner\n",
      "\n",
      "Tincoor, be 'd ay, ancs m G pomwovel ape st licholath k. wnt t t.\n",
      "\n",
      "MIVENThe\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(model.generate(idx)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5abe57",
   "metadata": {},
   "source": [
    "# Self attention tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61309d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "x = torch.randn((B,T,C))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f895c363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7236, -1.2030],\n",
       "        [-0.1934,  0.9959],\n",
       "        [-1.1941, -0.2165],\n",
       "        [-0.6883,  0.0201],\n",
       "        [-0.3822, -0.9448],\n",
       "        [-1.0139,  0.8695],\n",
       "        [ 0.8555, -1.4451],\n",
       "        [ 1.7656, -1.3940]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # (t,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1378b23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        prev = x[b, :t+1] # (t,c)\n",
    "        xbow[b,t] = prev.mean(dim=0) # (c,)\n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "833ab2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7236, -1.2030],\n",
       "        [ 0.7651, -0.1036],\n",
       "        [ 0.1120, -0.1412],\n",
       "        [-0.0880, -0.1009],\n",
       "        [-0.1469, -0.2697],\n",
       "        [-0.2914, -0.0798],\n",
       "        [-0.1275, -0.2749],\n",
       "        [ 0.1091, -0.4148]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xbow[0,1] == (x[0,0] + x[0,1])\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50fa8a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method 2\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei /= wei.sum(dim=1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69ba8397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7236, -1.2030],\n",
       "        [ 0.7651, -0.1036],\n",
       "        [ 0.1120, -0.1412],\n",
       "        [-0.0880, -0.1009],\n",
       "        [-0.1469, -0.2697],\n",
       "        [-0.2914, -0.0798],\n",
       "        [-0.1275, -0.2749],\n",
       "        [ 0.1091, -0.4148]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x # (t,t) @ (b,t,c) --> (b,t,c)\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd29c286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "545da062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method 3\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1051338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros(T,T)\n",
    "wei.masked_fill_(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57f2fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e43553aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae9c0f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention - 1 Head\n",
    "\n",
    "n_head = 16\n",
    "\n",
    "query = nn.Linear(C, n_head)\n",
    "key = nn.Linear(C, n_head)\n",
    "value = nn.Linear(C, n_head)\n",
    "q = query(x) # (b,t,c) @ (c,n_head) --> (b,t,n_head)\n",
    "# q[0][0]: (n_head,) --> query for 1st token in 1st sample in the batch\n",
    "\n",
    "k = key(x) # (b,t,c) @ (c,n_head) --> (b,t,n_head)\n",
    "\n",
    "wei = q @ k.transpose(-1,-2) # (b,t,n_head) @ (b,n_head,t) --> (b,t,t)\n",
    "# wei.shape\n",
    "\n",
    "wei = wei.masked_fill_(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x) # (b,t,n_head)\n",
    "out = wei @ v # (b,t,t) @ (b,t,n_head) --> (b,t,n_head)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33072ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2169,  2.7673,  4.0862,  3.2983,  2.4641,  4.1285,  0.1882, -1.3504],\n",
       "        [ 1.3089, -0.3266, -0.5393, -0.3296,  0.0983, -0.7367,  0.9044,  1.3835],\n",
       "        [ 0.5735, -0.9772, -4.1071, -2.8949, -3.2507, -2.6907, -1.3457,  0.4672],\n",
       "        [ 0.6178, -0.4946, -2.4876, -1.7053, -1.8949, -1.6097, -0.6265,  0.5569],\n",
       "        [-0.3196,  0.2502, -2.0488, -1.3006, -2.0667, -0.6804, -1.4232, -0.4891],\n",
       "        [ 1.4609, -1.2147, -3.0409, -2.1863, -1.8707, -2.5543,  0.0240,  1.4937],\n",
       "        [-1.1503,  1.8720,  1.3867,  1.3032,  0.2842,  2.2325, -0.8635, -1.3414],\n",
       "        [-1.3966,  2.8935,  4.1179,  3.3373,  2.3882,  4.2707,  0.0138, -1.5521]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617823b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8369, 0.1631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8187, 0.1737, 0.0076, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6796, 0.2234, 0.0304, 0.0666, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2862, 0.5059, 0.0508, 0.1073, 0.0499, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8622, 0.0594, 0.0096, 0.0225, 0.0308, 0.0156, 0.0000, 0.0000],\n",
       "        [0.0124, 0.2542, 0.1565, 0.1439, 0.0520, 0.3646, 0.0165, 0.0000],\n",
       "        [0.0013, 0.0943, 0.3207, 0.1469, 0.0569, 0.3736, 0.0053, 0.0011]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wei = wei.masked_fill_(tril==0, float('-inf'))\n",
    "# wei = F.softmax(wei, dim=-1)\n",
    "# wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5e6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

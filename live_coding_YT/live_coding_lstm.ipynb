{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c505f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bc2c1",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    def __init__(self, text, test_split=0.9):\n",
    "        self.vocab = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.stoi = {ch:i for i,ch in enumerate(self.vocab)}\n",
    "        self.itos = {i:ch for ch,i in self.stoi.items()}\n",
    "\n",
    "        self.data = [self.stoi[ch] for ch in text]\n",
    "        n = int(test_split * len(self.data))\n",
    "        self.train_data = self.data[:n]\n",
    "        self.val_data = self.data[n:]\n",
    "\n",
    "    def encode(self, text):\n",
    "        # return list of char indices\n",
    "        return [self.stoi[ch] for ch in text]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        # return string for the tokens list\n",
    "        return ''.join([self.itos[token] for token in tokens])\n",
    "\n",
    "    def get_batch(self, split, batch_size, block_size):\n",
    "        data = self.train_data if split=='train' else self.val_data\n",
    "        ix = torch.randint(0, len(data) - block_size, (batch_size,)) # [2,99, 56, 9000,...]\n",
    "        x = [data[i : i + block_size] for i in ix]\n",
    "        y = [data[i+1 : i+1 + block_size] for i in ix]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96066ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('../rnn_lstm/data/shakespeare.txt', 'r').read()\n",
    "loader = TextLoader(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loader.vocab)\n",
    "print(loader.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067130ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = loader.get_batch('train', 2, 4)\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e841e75",
   "metadata": {},
   "source": [
    "# Model construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    # prev_layer_hidden_state\n",
    "    def __init__(self, input_embd, hidden_embd):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        wx_gates: [wxf | wxi | wxg | wxo]\n",
    "        wh_gates: [whf | whi | whg | wxo]\n",
    "        b_gates : [bf | bi | bg | bo]\n",
    "        \"\"\"\n",
    "        self.wx_gates = nn.Parameter(torch.randn(input_embd, hidden_embd * 4) * 0.01)\n",
    "        self.bx_gates = nn.Parameter(torch.zeros(hidden_embd * 4))\n",
    "        self.wh_gates = nn.Parameter(torch.randn(hidden_embd, hidden_embd * 4) * 0.01)\n",
    "        self.bh_gates = nn.Parameter(torch.zeros(hidden_embd * 4))\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        # x: (B,n)\n",
    "        # layer_hidden: (B,hidden_embd)\n",
    "        # layer_cell: (B,hidden_embd)\n",
    "        x_gates = x @ self.wx_gates + self.bx_gates # (b,input_dim) @ (inpyt_embd, hidden_embd*4) -> (b,hidden_embd*4)\n",
    "        h_gates = h_prev @ self.wh_gates + self.bh_gates # (b,hidden_embd) @ (hidden_embd, hidden_embd*4) -> (b,hidden_embd*4)\n",
    "        gates_output = x_gates + h_gates\n",
    "\n",
    "        ft, it, gt, ot = gates_output.chunk(4, dim=1)\n",
    "\n",
    "        ft = torch.sigmoid(ft)\n",
    "        it = torch.sigmoid(it)\n",
    "        gt = torch.tanh(gt)\n",
    "        ot = torch.sigmoid(ot)\n",
    "\n",
    "        c_t = (ft * c_prev) + (it * gt)\n",
    "        h_t = ot * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "class MultiLayerLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, input_embd, hidden_embd, layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.hidden_embd = hidden_embd\n",
    "        self.embedding = nn.Parameter(torch.randn(vocab_size, input_embd) * 0.01)\n",
    "        self.lstm_layer = nn.ModuleList()\n",
    "        \n",
    "        self.lstm_layer.append(LSTMCell(input_embd, hidden_embd))\n",
    "        for layer in range(1, layers):\n",
    "            self.lstm_layer.append(LSTMCell(hidden_embd, hidden_embd))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout is not None and layers > 1 else None\n",
    "\n",
    "        self.why = nn.Parameter(torch.randn(hidden_embd, vocab_size) * 0.01)\n",
    "        self.by = nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T = x.shape\n",
    "        hs = torch.zeros(self.layers, B, self.hidden_embd, device=x.device)\n",
    "        cs = torch.zeros(self.layers, B, self.hidden_embd, device=x.device)\n",
    "        logits = []\n",
    "\n",
    "        emb = self.embedding[x] # (B,T,input_embd)\n",
    "        \n",
    "        for t in range(T):\n",
    "            xt = emb[:, t, :] # (B, input_embd)\n",
    "            hs_new = torch.zeros_like(hs, device=xt.device)\n",
    "            cs_new = torch.zeros_like(cs, device=xt.device)\n",
    "            for layer in range(self.layers):\n",
    "                h_layer, c_layer = hs[layer], cs[layer]\n",
    "                cell_layer = self.lstm_layer[layer]\n",
    "                h_new, c_new = cell_layer(xt, h_layer, c_layer)\n",
    "                hs_new[layer] = h_new\n",
    "                cs_new[layer] = c_new\n",
    "                if layer < self.layers - 1 and self.dropout is not None:\n",
    "                    xt = self.dropout(h_new)\n",
    "                else:\n",
    "                    xt = h_new\n",
    "            hs = hs_new\n",
    "            cs = cs_new\n",
    "            yt = hs[-1] @ self.why + self.by # (B, vocab_size)\n",
    "            logits.append(yt)\n",
    "        # now logits: T elements of shape (B, vocab_size)\n",
    "        logits = torch.stack(logits, dim=1) # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, stoi, itos, block_size, prompt=None, device='cpu', max_new_tokens=500, out_path='generated.txt'):\n",
    "    model.eval()\n",
    "    if not prompt:\n",
    "        idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        idx = torch.tensor([loader.stoi[ch] for ch in prompt], dtype=torch.long, device=device)\n",
    "    generated_chars = []\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cropped = idx[:, -block_size:] # (b,T)\n",
    "        logits = model(idx_cropped) # (b,T,vocab_size)\n",
    "        logits = logits[0][-1] # (vocab_size,) vector from last time step\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        generated_chars.append(loader.itos[next_idx])\n",
    "    full_text = ''.join(generated_chars)\n",
    "    print(full_text)\n",
    "    # with open(out_path, 'w', encoding='utf-8') as fp:\n",
    "    #     fp.write(full_text)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a203f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 8\n",
    "n_hidden = 32\n",
    "device = 'mps'\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "model = MultiLayerLSTM(loader.vocab_size, input_embd=n_embd, hidden_embd=n_hidden, layers=2)\n",
    "model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "generate(model, loader.stoi, loader.itos, block_size=block_size, device='mps', max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32\n",
    "n_hidden = 32\n",
    "device = 'mps'\n",
    "batch_size = 8\n",
    "block_size = 32\n",
    "\n",
    "model = MultiLayerLSTM(loader.vocab_size, input_embd=n_embd, hidden_embd=n_hidden, layers=2)\n",
    "model.to(device)\n",
    "print(f'{sum(p.numel() for p in model.parameters())} params')\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "max_iters = 2000\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for i in range(max_iters):\n",
    "    x, y = loader.get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    B,T = x.shape\n",
    "    # forward pass\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(B*T, -1), y.view(B*T))\n",
    "\n",
    "    # backward pass\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    optim.step()\n",
    "\n",
    "    # validation\n",
    "    if i % 200 == 0:\n",
    "        with torch.no_grad():\n",
    "            x_val, y_val = loader.get_batch('val', batch_size=512, block_size=block_size)\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            logits = model(x_val)\n",
    "            val_loss = F.cross_entropy(logits.view(-1, loader.vocab_size), y_val.view(-1))\n",
    "        # early-stopping\n",
    "        if val_loss < best_val_loss - 1e-4: # small delta to be considered\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve > patience:\n",
    "            print(f'Early stop @ epoch {i}. \\n__________________Best validation loss = {best_val_loss:.4f}')\n",
    "            break\n",
    "    if i % 200 == 0:\n",
    "        print(f'Iteration {i} | train loss = {loss.item():.4f} | val loss = {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafe0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, loader.stoi, loader.itos, block_size=block_size, device='mps', max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93364687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

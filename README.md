my work towards building a GPT2 type model

1. Build from ground up
    - count based bigram prediction
    - neural network bigram prediction
    - multi-layer perceptron next char prediction
    - wavenet next char prediction
    - GPT char based text generation
    - GPT2: recreate

2. I post live coding to my YT channel: 
